{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d0fcc9d-dbb6-4783-a8bb-b5cefb76e263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mosaic AI Agent Framework: Author and deploy an MCP tool-calling OpenAI Responses API agent\n",
    "\n",
    "https://docs.databricks.com/aws/en/notebooks/source/generative-ai/openai-mcp-tool-calling-agent.html\n",
    "\n",
    "This notebook shows how to author an OpenAI agent that connects to MCP servers hosted on Databricks. You can connect to Databricks-managed MCP servers, custom MCP servers hosted as a Databricks App, or both simultaneously. To learn more about these options, see [MCP on Databricks](https://docs.databricks.com/aws/en/generative-ai/mcp/).\n",
    "\n",
    "\n",
    "This notebook uses the [`ResponsesAgent`](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ResponsesAgent) interface for compatibility with Mosaic AI features. In this notebook you learn to:\n",
    "\n",
    "- Author an [Open AI Responses API](https://platform.openai.com/docs/api-reference/responses) agent (wrapped with `ResponsesAgent`) that calls MCP tools\n",
    "- Manually test the agent\n",
    "- Evaluate the agent using Mosaic AI Agent Evaluation\n",
    "- Log and deploy the agent\n",
    "\n",
    "To learn more about authoring an agent using Mosaic AI Agent Framework, see Databricks documentation ([AWS](https://docs.databricks.com/aws/generative-ai/agent-framework/author-agent) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/create-chat-model)).\n",
    "\n",
    "## Prerequisites\n",
    "- Address all `TODO`s in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66837e5c-0d42-4d9c-b483-57b1c503c7d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq backoff databricks-openai openai uv databricks-agents>=1.0.0 databricks-mcp\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "424dbeed-6f5d-4462-86bf-062be45250c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Define the agent in code\n",
    "Define the agent code in a single cell below. This lets you easily write the agent code to a local Python file, using the `%%writefile` magic command, for subsequent logging and deployment.\n",
    "\n",
    "#### Agent tools\n",
    "This agent code adds the built-in Unity Catalog function `system.ai.python_exec` to the agent. The agent code also includes commented-out sample code for adding a vector search index to perform unstructured data retrieval.\n",
    "\n",
    "For more examples of tools to add to your agent, see Databricks documentation ([AWS](https://docs.databricks.com/aws/generative-ai/agent-framework/agent-tool) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/agent-tool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62aeecf0-0795-4df5-b021-937ce5fc60f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from typing import Any, Callable, Generator, List, Optional\n",
    "from uuid import uuid4\n",
    "\n",
    "import backoff\n",
    "import mlflow\n",
    "import nest_asyncio\n",
    "import openai\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_mcp import DatabricksMCPClient, DatabricksOAuthClientProvider\n",
    "from databricks_ai_bridge import ModelServingUserCredentials\n",
    "\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client as connect\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "############################################\n",
    "## Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that have access to tools.\n",
    "\"\"\"\n",
    "\n",
    "###############################################################################\n",
    "# Configure Custom Tools for your agent\n",
    "###############################################################################\n",
    "\n",
    "# TODO:Define execution function and function spec for each custom tool\n",
    "\n",
    "@mlflow.trace(name=\"parse_inputs\", span_type=SpanType.TOOL)\n",
    "def _parse_inputs(s: str) -> dict:\n",
    "    a_str, b_str = s.strip().split()\n",
    "    return {\"a\": int(a_str), \"b\": int(b_str)}\n",
    "\n",
    "@mlflow.trace(name=\"add_numbers\", span_type=SpanType.TOOL)\n",
    "def _add_numbers(p: dict) -> dict:\n",
    "    return {\"sum\": p[\"a\"] + p[\"b\"]}\n",
    "\n",
    "@mlflow.trace(name=\"double_result\", span_type=SpanType.TOOL)\n",
    "def _double_result(p: dict) -> int:\n",
    "    return p[\"sum\"] * 2\n",
    "\n",
    "def streaming_adder_exec(**kwargs):\n",
    "    \"\"\"Generator that streams each step, then returns final summary.\"\"\"\n",
    "    text = kwargs.get(\"text\", \"\").strip()\n",
    "    p1 = _parse_inputs(text); yield f\"parsed: {p1}\"\n",
    "    p2 = _add_numbers(p1);    yield f\"sum: {p2['sum']}\"\n",
    "    out = _double_result(p2); yield f\"double: {out}\"\n",
    "    return f\"parsed: {p1}\\nsum: {p2['sum']}\\ndouble: {out}\"\n",
    "\n",
    "STREAMING_ADDER_SPEC = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"streaming_adder\",\n",
    "        \"description\": \"Parse 'a b', add, then double the result.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"text\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"text\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "###############################################################################\n",
    "# Configure MCP Servers for your agent\n",
    "# This section sets up server connections so your agent can retrieve data or take actions.\n",
    "###############################################################################\n",
    "\n",
    "#### TODO: Choose your MCP server connection type.\n",
    "\n",
    "# ----- Simple: Managed MCP Server (no extra setup required) -----\n",
    "# Uses your Databricks Workspace settings and Personal Access Token (PAT) auth.\n",
    "# workspace_client = WorkspaceClient()\n",
    "\n",
    "# Managed MCP Servers: Ready to use with default settings above\n",
    "host = \"https://e2-demo-field-eng.cloud.databricks.com\"\n",
    "MANAGED_MCP_SERVER_URLS = [\n",
    "    f\"{host}/api/2.0/mcp/functions/system/ai\",\n",
    "]\n",
    "\n",
    "# ----- Advanced (optional): Custom MCP Server with OAuth -----\n",
    "# For Databricks Apps hosting custom MCP servers, OAuth with a service principal is required.\n",
    "# Uncomment and fill in your settings ONLY if connecting to a custom MCP server.\n",
    "\n",
    "import os\n",
    "workspace_client = WorkspaceClient(\n",
    "    host= host,\n",
    "    client_id=os.getenv(\"DATABRICKS_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"DATABRICKS_CLIENT_SECRET\"),\n",
    "    auth_type=\"oauth-m2m\"\n",
    ")\n",
    "\n",
    "# Add custom MCP servers here if needed\n",
    "app_url =\"https://databricks-mcp-server-1444828305810485.aws.databricksapps.com/mcp/\"\n",
    "\n",
    "# Custom MCP Servers: Add URLs below if needed (requires custom setup and OAuth above)\n",
    "CUSTOM_MCP_SERVER_URLS = [\n",
    "    app_url\n",
    "    # e.g. \"https://<custom-mcp-url>/mcp\"\n",
    "]\n",
    "\n",
    "\n",
    "class ToolInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    Class representing a tool for the agent.\n",
    "    - \"name\" (str): The name of the tool.\n",
    "    - \"spec\" (dict): JSON description of the tool (matches OpenAI Responses format)\n",
    "    - \"exec_fn\" (Callable): Function that implements the tool logic\n",
    "    \"\"\"\n",
    "\n",
    "    name: str\n",
    "    spec: dict\n",
    "    exec_fn: Callable\n",
    "\n",
    "\n",
    "async def _run_custom_async(server_url: str, tool_name: str, ws: WorkspaceClient, **kwargs) -> str:\n",
    "    \"\"\"Executes a tool from a custom MCP server asynchronously using OAuth.\"\"\"\n",
    "    async with connect(server_url, auth=DatabricksOAuthClientProvider(ws)) as (\n",
    "        read_stream,\n",
    "        write_stream,\n",
    "        _,\n",
    "    ):\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            await session.initialize()\n",
    "            response = await session.call_tool(tool_name, kwargs)\n",
    "            return \"\".join([c.text for c in response.content])\n",
    "\n",
    "\n",
    "async def get_custom_mcp_tools(ws: WorkspaceClient, server_url: str):\n",
    "    \"\"\"Retrieves the list of tools available from a custom MCP server.\"\"\"\n",
    "    async with connect(server_url, auth=DatabricksOAuthClientProvider(ws)) as (\n",
    "        read_stream,\n",
    "        write_stream,\n",
    "        _,\n",
    "    ):\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            await session.initialize()\n",
    "            tools_response = await session.list_tools()\n",
    "            return tools_response.tools\n",
    "\n",
    "\n",
    "async def create_mcp_tools(\n",
    "    ws: WorkspaceClient, managed_server_urls: List[str] = None, custom_server_urls: List[str] = None\n",
    ") -> List[ToolInfo]:\n",
    "    \"\"\"Aggregates all available tools from both managed and custom MCP servers into OpenAI-compatible ToolInfo objects.\"\"\"\n",
    "    tools = []\n",
    "\n",
    "    #### TODO: uncomment these to use obo client for obo auth. ####\n",
    "    #### use Ouath client for custom apps MCP server, or PAT for only managed MCP or no MCP ####\n",
    "    # obo_ws_client = WorkspaceClient(credentials_strategy=ModelServingUserCredentials())\n",
    "    # ws = obo_ws_client\n",
    "\n",
    "\n",
    "    if managed_server_urls:\n",
    "        for server_url in managed_server_urls:\n",
    "            try:\n",
    "                mcp_client = DatabricksMCPClient(server_url=server_url, workspace_client=ws)\n",
    "                mcp_tools = mcp_client.list_tools()\n",
    "\n",
    "                for mcp_tool in mcp_tools:\n",
    "                    tool_spec = {\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": mcp_tool.name,\n",
    "                            \"parameters\": mcp_tool.inputSchema,\n",
    "                        },\n",
    "                        \"description\": mcp_tool.description or f\"Tool: {mcp_tool.name}\",\n",
    "                    }\n",
    "\n",
    "                    def create_managed_exec_fn(server_url, tool_name, ws):\n",
    "                        def exec_fn(**kwargs):\n",
    "                            client = DatabricksMCPClient(server_url=server_url, workspace_client=ws)\n",
    "                            response = client.call_tool(tool_name, kwargs)\n",
    "                            return \"\".join([c.text for c in response.content])\n",
    "\n",
    "                        return exec_fn\n",
    "\n",
    "                    exec_fn = create_managed_exec_fn(server_url, mcp_tool.name, ws)\n",
    "\n",
    "                    tools.append(ToolInfo(name=mcp_tool.name, spec=tool_spec, exec_fn=exec_fn))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tools from managed server {server_url}: {e}\")\n",
    "\n",
    "    if custom_server_urls:\n",
    "        for server_url in custom_server_urls:\n",
    "            try:\n",
    "                mcp_tools = await get_custom_mcp_tools(ws, server_url)\n",
    "\n",
    "                for mcp_tool in mcp_tools:\n",
    "                    tool_spec = {\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": mcp_tool.name,\n",
    "                            \"parameters\": mcp_tool.inputSchema,\n",
    "                        },\n",
    "                        \"description\": mcp_tool.description or f\"Tool: {mcp_tool.name}\",\n",
    "                    }\n",
    "\n",
    "                    def create_custom_exec_fn(server_url, tool_name, ws):\n",
    "                        def exec_fn(**kwargs):\n",
    "                            return asyncio.run(\n",
    "                                _run_custom_async(server_url, tool_name, ws, **kwargs)\n",
    "                            )\n",
    "\n",
    "                        return exec_fn\n",
    "\n",
    "                    exec_fn = create_custom_exec_fn(server_url, mcp_tool.name, ws)\n",
    "\n",
    "                    tools.append(ToolInfo(name=mcp_tool.name, spec=tool_spec, exec_fn=exec_fn))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tools from custom server {server_url}: {e}\")\n",
    "\n",
    "    return tools\n",
    "\n",
    "\n",
    "class MCPToolCallingAgent(ResponsesAgent):\n",
    "    def __init__(self, llm_endpoint: str, tools: list[ToolInfo]):\n",
    "        \"\"\"Initializes the MCP Tool Calling Agent.\"\"\"\n",
    "        self.llm_endpoint = llm_endpoint\n",
    "        self.workspace_client = workspace_client# WorkspaceClient()\n",
    "        self.model_serving_client = self.workspace_client.serving_endpoints.get_open_ai_client()\n",
    "        self._tools_dict = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def get_tool_specs(self) -> list[dict]:\n",
    "        \"\"\"Returns tool specifications in the format OpenAI expects.\"\"\"\n",
    "        return [tool_info.spec for tool_info in self._tools_dict.values()]\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.TOOL)\n",
    "    def execute_tool(self, tool_name: str, args: dict) -> Any:\n",
    "        \"\"\"Executes the specified tool with the given arguments.\"\"\"\n",
    "        return self._tools_dict[tool_name].exec_fn(**args)\n",
    "\n",
    "    @backoff.on_exception(backoff.expo, openai.RateLimitError)\n",
    "    @mlflow.trace(span_type=SpanType.LLM)\n",
    "    def call_llm(self, messages: list[dict[str, Any]]) -> Generator[dict[str, Any], None, None]:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"PydanticSerializationUnexpectedValue\")\n",
    "            for chunk in self.model_serving_client.chat.completions.create(\n",
    "                model=self.llm_endpoint,\n",
    "                messages=self.prep_msgs_for_cc_llm(messages),\n",
    "                tools=self.get_tool_specs(),\n",
    "                stream=True,\n",
    "            ):\n",
    "                yield chunk.to_dict()\n",
    "\n",
    "    # def handle_tool_call(\n",
    "    #     self, tool_call: dict[str, Any], messages: list[dict[str, Any]]\n",
    "    # ) -> ResponsesAgentStreamEvent:\n",
    "    #     \"\"\"\n",
    "    #     Execute tool calls, add them to the running message history, and return a ResponsesStreamEvent w/ tool output\n",
    "    #     \"\"\"\n",
    "    #     args = json.loads(tool_call[\"arguments\"])\n",
    "    #     result = str(self.execute_tool(tool_name=tool_call[\"name\"], args=args))\n",
    "\n",
    "    #     tool_call_output = self.create_function_call_output_item(tool_call[\"call_id\"], result)\n",
    "    #     messages.append(tool_call_output)\n",
    "    #     return ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=tool_call_output)\n",
    "\n",
    "    # TODO: Updated function to handle tool outputs (this now returns a generator to accomodate step-by-step output from custom tool)\n",
    "    def handle_tool_call(self, tool_call: dict[str, Any], messages: list[dict[str, Any]]) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        name, args, call_id = tool_call[\"name\"], json.loads(tool_call[\"arguments\"] or \"{}\"), tool_call[\"call_id\"]\n",
    "        result = self.execute_tool(name, args)\n",
    "        import inspect\n",
    "        if inspect.isgenerator(result):\n",
    "            chunks = []\n",
    "            try:\n",
    "                while True:\n",
    "                    chunk = next(result)\n",
    "                    chunks.append(str(chunk))\n",
    "                    yield ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=self.create_function_call_output_item(call_id, str(chunk)))\n",
    "            except StopIteration as stop:\n",
    "                final_text = stop.value if stop.value is not None else \"\\n\".join(chunks)\n",
    "        else:\n",
    "            final_text = str(result)\n",
    "        final_item = self.create_function_call_output_item(call_id, final_text)\n",
    "        messages.append(\n",
    "        final_item.model_dump() if hasattr(final_item, \"model_dump\") else final_item\n",
    "    )\n",
    "        yield ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=final_item)\n",
    "\n",
    "    def call_and_run_tools(\n",
    "        self,\n",
    "        messages: list[dict[str, Any]],\n",
    "        max_iter: int = 10,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        for _ in range(max_iter):\n",
    "            last_msg = messages[-1]\n",
    "            if last_msg.get(\"role\", None) == \"assistant\":\n",
    "                return\n",
    "            elif last_msg.get(\"type\", None) == \"function_call\":\n",
    "                yield from self.handle_tool_call(last_msg, messages)\n",
    "            else:\n",
    "                yield from self.output_to_responses_items_stream(\n",
    "                    chunks=self.call_llm(messages), aggregator=messages\n",
    "                )\n",
    "\n",
    "        yield ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item=self.create_text_output_item(\"Max iterations reached. Stopping.\", str(uuid4())),\n",
    "        )\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] + [\n",
    "            i.model_dump() for i in request.input\n",
    "        ]\n",
    "\n",
    "        yield from self.call_and_run_tools(messages)\n",
    "\n",
    "\n",
    "# Create MCP tools from the configured servers\n",
    "mcp_tools = asyncio.run(\n",
    "    create_mcp_tools(\n",
    "        ws=workspace_client,\n",
    "        managed_server_urls=MANAGED_MCP_SERVER_URLS,\n",
    "        custom_server_urls=CUSTOM_MCP_SERVER_URLS,\n",
    "    )\n",
    ")\n",
    "\n",
    "#### TODO: add custom tool here ####\n",
    "mcp_tools.append(ToolInfo(name=\"streaming_adder\", spec=STREAMING_ADDER_SPEC, exec_fn=streaming_adder_exec))\n",
    "\n",
    "# Log the model using MLflow\n",
    "mlflow.openai.autolog()\n",
    "AGENT = MCPToolCallingAgent(llm_endpoint=LLM_ENDPOINT_NAME, tools=mcp_tools)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0de7aebd-daca-40ee-8008-e277f99e57f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output. Since we manually traced methods within `ResponsesAgent`, you can view the trace for each step the agent takes, with any LLM calls made via the OpenAI SDK automatically traced by autologging.\n",
    "\n",
    "Replace this placeholder input with an appropriate domain-specific example for your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf131f40-d7d6-44df-8e8f-aff43e458150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa3a25f-3f68-4220-8ed5-db3c7b71eb75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TODO: ONLY UNCOMMENT AND EDIT THIS SECTION IF YOU ARE USING OAUTH/SERVICE PRINCIPAL FOR CUSTOM MCP SERVERS.\n",
    "#       For managed MCP (the default), LEAVE THIS SECTION COMMENTED OUT.\n",
    "# ==============================================================================\n",
    "\n",
    "# # Set your Databricks client ID and client secret for service principal authentication.\n",
    "# DATABRICKS_CLIENT_ID = \"<YOUR_CLIENT_ID>\"\n",
    "# client_secret_scope_name = \"<YOUR_SECRET_SCOPE>\"\n",
    "# client_secret_key_name = \"<YOUR_SECRET_KEY_NAME>\"\n",
    "\n",
    "# # Load your service principal credentials into environment variables\n",
    "# os.environ[\"DATABRICKS_CLIENT_ID\"] = DATABRICKS_CLIENT_ID\n",
    "# os.environ[\"DATABRICKS_CLIENT_SECRET\"] = dbutils.secrets.get(scope=client_secret_scope_name, key=client_secret_key_name)\n",
    "\n",
    "import os\n",
    "client_secret_scope_name = \"cindy_demos\"\n",
    "client_secret_key_name = \"cindy_demo_service_principal_secret\"\n",
    "client_id_key_name = \"cindy_demo_service_principal_client\"\n",
    "os.environ[\"DATABRICKS_CLIENT_ID\"] = dbutils.secrets.get(scope=client_secret_scope_name, key=client_id_key_name)\n",
    "os.environ[\"DATABRICKS_CLIENT_SECRET\"] = dbutils.secrets.get(scope=client_secret_scope_name, key=client_secret_key_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b511817a-e442-47be-990e-b9d8d81c216a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "result = AGENT.predict({\"input\": [{\"role\": \"user\", \"content\": \"What is 6*7 in Python\"}]})\n",
    "print(result.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762feee8-3795-4212-8914-dfdf489f5607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "req = {\"input\": [{\"role\": \"user\", \"content\": \"What is 6 7 with streaming adder\"}]}\n",
    "for ev in AGENT.predict_stream(req):\n",
    "    if ev.type == \"response.output_item.done\":\n",
    "        print('TIME:', time.time())\n",
    "        print('EVENT:', ev.item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc73ad02-4132-4442-945f-4fedac6c2990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for chunk in AGENT.predict_stream(\n",
    "    {\"input\": [{\"role\": \"user\", \"content\": \"6 7 with adder\"}]}\n",
    "):\n",
    "    print(chunk.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade67d93-27ff-433c-965c-c41fa7ccebae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent as an MLflow model\n",
    "\n",
    "Log the agent as code from the `agent.py` file. See [MLflow - Models from Code](https://mlflow.org/docs/latest/models.html#models-from-code).\n",
    "\n",
    "### Enable automatic authentication for Databricks resources\n",
    "For the most common Databricks resource types, Databricks supports and recommends declaring resource dependencies for the agent upfront during logging. This enables automatic authentication passthrough when you deploy the agent. With automatic authentication passthrough, Databricks automatically provisions, rotates, and manages short-lived credentials to securely access these resource dependencies from within the agent endpoint.\n",
    "\n",
    "To enable automatic authentication, specify the dependent Databricks resources when calling `mlflow.pyfunc.log_model().`\n",
    "\n",
    "  - **TODO**: If your Unity Catalog tool queries a [vector search index](docs link) or leverages [external functions](docs link), you need to include the dependent vector search index and UC connection objects, respectively, as resources. See docs ([AWS](https://docs.databricks.com/generative-ai/agent-framework/log-agent.html#specify-resources-for-automatic-authentication-passthrough) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/log-agent#resources)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e12bb4b4-7720-4e91-aed8-a400fa5b0925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import MANAGED_MCP_SERVER_URLS, CUSTOM_MCP_SERVER_URLS, workspace_client, LLM_ENDPOINT_NAME\n",
    "from mlflow.models.resources import DatabricksServingEndpoint, DatabricksFunction, DatabricksApp\n",
    "from databricks_mcp import DatabricksOAuthClientProvider, DatabricksMCPClient\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "### Agent Auth (Agent is using a SP)\n",
    "app_name = \"databricks-mcp-server\"\n",
    "\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME),\n",
    "    DatabricksApp(app_name=app_name)]\n",
    "\n",
    "for mcp_server_url in MANAGED_MCP_SERVER_URLS:\n",
    "    mcp_client = DatabricksMCPClient(server_url=mcp_server_url, workspace_client=workspace_client)\n",
    "    resources.extend(mcp_client.get_databricks_resources())\n",
    "\n",
    "resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b3f9551-73e8-4ef1-9331-731571e1a893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            f\"databricks-mcp=={get_distribution('databricks-mcp').version}\",\n",
    "            \"backoff\",\n",
    "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
    "            f\"mcp=={get_distribution('mcp').version}\",\n",
    "            f\"databricks-openai=={get_distribution('databricks-openai').version}\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54c05ed5-7728-4803-af62-74ff1aea600b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the agent with Agent Evaluation\n",
    "\n",
    "Use Mosaic AI Agent Evaluation to evalaute the agent's responses based on expected responses and other evaluation criteria. Use the evaluation criteria you specify to guide iterations, using MLflow to track the computed quality metrics.\n",
    "See Databricks documentation ([AWS]((https://docs.databricks.com/aws/generative-ai/agent-evaluation) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-evaluation/)).\n",
    "\n",
    "\n",
    "To evaluate your tool calls, add custom metrics. See Databricks documentation ([AWS](https://docs.databricks.com/en/generative-ai/agent-evaluation/custom-metrics.html#evaluating-tool-calls) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-evaluation/custom-metrics#evaluating-tool-calls))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "609499e6-afcb-43c9-8330-8611c4edb6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import RelevanceToQuery, RetrievalGroundedness, RetrievalRelevance, Safety\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"input\": [{\"role\": \"user\", \"content\": \"what is 6 7 based on the adder tool.\"}]},\n",
    "        \"expected_response\": \"The final result is 26.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=lambda input: AGENT.predict({\"input\": input}),\n",
    "    scorers=[RelevanceToQuery(), Safety()],  # add more scorers here if they're applicable\n",
    ")\n",
    "\n",
    "# Review the evaluation results in the MLfLow UI (see console output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f463e1d-aeac-48a9-b6df-a29e4f1e62e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pre-deployment agent validation\n",
    "Before registering and deploying the agent, perform pre-deployment checks using the [mlflow.models.predict()](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.predict) API. See Databricks documentation ([AWS](https://docs.databricks.com/en/machine-learning/model-serving/model-serving-debug.html#validate-inputs) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/model-serving-debug#before-model-deployment-validation-checks))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92627265-a9bf-46d9-8b2f-7aba38d4742f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"Hello!\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c11f6e8-d7d7-4424-8b91-bc8dde0a5089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the model to Unity Catalog\n",
    "\n",
    "Before you deploy the agent, you must register the agent to Unity Catalog.\n",
    "\n",
    "- **TODO** Update the `catalog`, `schema`, and `model_name` below to register the MLflow model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4dd57ac-2e57-44ac-8e20-80bad9268489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"cindy_demo_catalog\"\n",
    "schema = \"default\"\n",
    "model_name = \"mcp_tool_agent\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4405f486-994e-4776-b131-71d4636a5e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50009a39-e63f-4ca6-9000-6ff828c5eb1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME, \n",
    "    uc_registered_model_info.version,\n",
    "    environment_vars={\n",
    "    # ==============================================================================\n",
    "    # TODO: ONLY UNCOMMENT AND CONFIGURE THE ENVIRONMENT_VARS SECTION BELOW\n",
    "    #       IF YOU ARE USING OAUTH/SERVICE PRINCIPAL FOR CUSTOM MCP SERVERS.\n",
    "    #       For managed MCP (the default), LEAVE THIS SECTION COMMENTED OUT.\n",
    "    # ==============================================================================\n",
    "        \"DATABRICKS_CLIENT_ID\": f\"{{{{secrets/{client_secret_scope_name}/{client_id_key_name}}}}}\",\n",
    "        \"DATABRICKS_CLIENT_SECRET\": f\"{{{{secrets/{client_secret_scope_name}/{client_secret_key_name}}}}}\"\n",
    "    },\n",
    "    tags = {\"endpointSource\": \"docs\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a50a3b-0a21-423b-bb9a-38b7d90f8c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent endpoint\n",
    "\n",
    "After your agent is deployed, you can chat with it in AI playground to perform additional checks, share it with SMEs in your organization for feedback, or embed it in a production application. See docs ([AWS](https://docs.databricks.com/en/generative-ai/deploy-agent.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/deploy-agent)) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2282c12-7e5f-4244-b4ac-662f9524226a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "endpoint = \"agents_cindy_demo_catalog-default-mcp_tool_agent\"\n",
    "\n",
    "w = WorkspaceClient()  # Ensure databricks-sdk[openai] is installed\n",
    "client = w.serving_endpoints.get_open_ai_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f180110-f769-41c3-82f8-38d741034fc8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Light parsing and formatting"
    }
   },
   "outputs": [],
   "source": [
    "streaming_resp = client.responses.create(\n",
    "    model=endpoint,\n",
    "    stream= True,\n",
    "    input=[{\"role\": \"user\", \"content\": \"What is answer for 12 12 with streaming adder\"}],\n",
    "    extra_body = {\"databricks_options\": {\"return_trace\": True}} # returns detailed traces\n",
    ")\n",
    "for chunk in streaming_resp:\n",
    "  if chunk.type == \"response.output_item.done\":\n",
    "    if chunk.item.type=='function_call_output':\n",
    "      print('TOOL OUTPUT:',chunk.item.output)\n",
    "    if chunk.item.type=='message':\n",
    "      print('AGENT OUPUT:',chunk.item.content[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70f113b6-2b49-42b8-90b2-5c6a8bff272d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "no formatting"
    }
   },
   "outputs": [],
   "source": [
    "streaming_resp = client.responses.create(\n",
    "    model=endpoint,\n",
    "    stream= True,\n",
    "    input=[{\"role\": \"user\", \"content\": \"what tools do you have\"}],\n",
    "    extra_body = {\"databricks_options\": {\"return_trace\": True}}\n",
    ")\n",
    "for chunk in streaming_resp:\n",
    "  if chunk.type == \"response.output_item.done\":\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52f6bb15-4137-4582-b4d9-9778a793c5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunk#.item.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "784fb68e-69be-4c3d-87eb-c26365583860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2c_openai-mcp-tool-calling-agent-streaming-output",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
